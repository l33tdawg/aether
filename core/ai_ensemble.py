"""
Enhanced AI Ensemble Module for AetherAudit
Implements specialized agents with local consensus suitable for tests.
"""

import asyncio
import json
import sqlite3
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
import logging
import hashlib

from core.database_manager import DatabaseManager, LearningPattern, AuditMetrics
from core.config_manager import ConfigManager
from core.json_utils import parse_llm_json

logger = logging.getLogger(__name__)

@dataclass
class ModelResult:
    """Result from a single AI model"""
    model_name: str
    findings: List[Dict[str, Any]]
    confidence: float
    processing_time: float
    metadata: Dict[str, Any]

@dataclass
class ConsensusResult:
    """Consensus result from multiple models"""
    consensus_findings: List[Dict[str, Any]]
    model_agreement: float
    confidence_score: float
    processing_time: float
    individual_results: List[ModelResult]

def _get_analysis_model() -> str:
    """Get the analysis model from config (supports mixed OpenAI/Gemini)."""
    try:
        from core.config_manager import get_model_for_task
        return get_model_for_task('analysis')
    except Exception:
        return 'gpt-5-mini'  # Fallback

class BaseAIModel:
    """Base class for specialized AI agents"""

    def __init__(self, agent_name: str, role: str, focus_areas: List[str], confidence_weight: float = 1.0):
        self.agent_name = agent_name
        self.role = role
        self.focus_areas = focus_areas
        self.confidence_weight = confidence_weight
        self.db_manager = DatabaseManager()
        self.config = ConfigManager()
    
    async def analyze_contract(self, contract_content: str, contract_path: str = "", context: Dict[str, Any] = None) -> ModelResult:
        """Analyze contract and return results with learning integration"""
        raise NotImplementedError
    
    def get_focus_areas(self) -> List[str]:
        """Get agent's focus areas"""
        return self.focus_areas

    def _get_persona_prompt(self) -> str:
        """Get the persona-specific system prompt"""
        raise NotImplementedError

    def _apply_learning_patterns(self, contract_content: str) -> Dict[str, Any]:
        """Apply learned patterns from database to improve analysis"""
        try:
            # Get relevant learning patterns for this agent's focus areas
            context_enhancements = {
                'learned_patterns': [],
                'confidence_adjustments': {},
                'focus_areas': self.focus_areas
            }

            for focus_area in self.focus_areas:
                # Compatibility: prefer get_learning_patterns(pattern_type=..)
                # Fallback: if missing, skip gracefully
                patterns = []
                try:
                    if hasattr(self.db_manager, 'get_learning_patterns'):
                        patterns = self.db_manager.get_learning_patterns(pattern_type=focus_area) or []
                except Exception:
                    patterns = []
                for pattern in patterns:
                    if pattern.get('success_rate', 0) > 0.7:  # Only use high-confidence patterns
                        context_enhancements['learned_patterns'].append({
                            'type': pattern.get('pattern_type', ''),
                            'original': pattern.get('original_classification', ''),
                            'corrected': pattern.get('corrected_classification', ''),
                            'confidence_threshold': pattern.get('confidence_threshold', 0.8),
                            'reasoning': pattern.get('reasoning', '')
                        })

            return context_enhancements
        except Exception as e:
            logger.warning(f"Failed to apply learning patterns: {e}")
            return {'learned_patterns': [], 'confidence_adjustments': {}, 'focus_areas': self.focus_areas}

    def _store_analysis_result(self, result: ModelResult, contract_hash: str):
        """Store analysis result for learning"""
        try:
            # Store metrics for this analysis
            metrics = AuditMetrics(
                id=f"ensemble_{self.agent_name}_{int(time.time())}",
                audit_result_id=contract_hash,
                total_findings=len(result.findings),
                confirmed_findings=0,  # Will be updated later
                false_positives=0,
                accuracy_score=result.confidence,
                precision_score=0.0,
                recall_score=0.0,
                f1_score=0.0,
                execution_time=result.processing_time,
                llm_calls=1,
                cache_hits=0,
                created_at=time.time()
            )
            # Compatibility: prefer store_audit_metrics; fallback to save_audit_metrics
            if hasattr(self.db_manager, 'store_audit_metrics'):
                self.db_manager.store_audit_metrics(metrics)
            elif hasattr(self.db_manager, 'save_audit_metrics'):
                self.db_manager.save_audit_metrics(metrics)
            else:
                raise AttributeError("DatabaseManager missing audit metrics methods")
        except Exception as e:
            logger.warning(f"Failed to store analysis result: {e}")


class DeFiSecurityExpert(BaseAIModel):
    """DeFi Protocol Security Expert - Focuses on DeFi-specific vulnerabilities"""

    def __init__(self):
        super().__init__(
            agent_name="defi_expert",
            role="DeFi Protocol Security Expert",
            focus_areas=["defi", "flash_loans", "yield_farming", "dex", "lending", "staking"]
        )

    def _get_persona_prompt(self) -> str:
        return """You are a **DeFi Protocol Security Expert** with deep expertise in decentralized finance vulnerabilities.

**Your Mission:** Identify DeFi-specific security risks that could lead to fund losses or protocol exploits.

**Focus Areas:**
- **Flash Loan Attacks**: Complex multi-step attacks using flash loans
- **Yield Farming Exploits**: Manipulation of reward distribution and staking mechanisms
- **DEX Vulnerabilities**: Price oracle manipulation, front-running, sandwich attacks
- **Lending Protocol Risks**: Liquidation manipulation, oracle failures, governance attacks
- **Staking Contract Issues**: Reward calculation errors, withdrawal vulnerabilities

**Analysis Approach:**
1. **Pattern Recognition**: Look for DeFi-specific attack patterns and anti-patterns
2. **Economic Analysis**: Consider incentive structures and economic attack vectors
3. **Protocol Integration**: Analyze how this contract interacts with other DeFi protocols
4. **Governance Risks**: Check for admin key management and upgrade mechanism security

**Output Format:** JSON array of findings with DeFi-specific context"""

    async def analyze_contract(self, contract_content: str, contract_path: str = "", context: Dict[str, Any] = None) -> ModelResult:
        """Analyze contract from DeFi security perspective"""
        start_time = time.time()

        try:
            # Apply learning patterns
            learning_context = self._apply_learning_patterns(contract_content)

            # Create contract hash for tracking
            contract_hash = hashlib.md5(contract_content.encode()).hexdigest()

            # Build specialized prompt
            prompt = f"""
{self._get_persona_prompt()}

**CONTRACT TO ANALYZE:**
```solidity
{contract_content}
```

**LEARNING CONTEXT:**
{json.dumps(learning_context, indent=2)}

**REQUIRED OUTPUT:**
Return a JSON array of vulnerabilities found. Each finding should include:
- type: Specific vulnerability type (e.g., "flash_loan_arbitrage", "oracle_manipulation")
- severity: "low" | "medium" | "high" | "critical"
- confidence: 0.0-1.0 (how certain you are about this finding)
- description: Detailed explanation of the vulnerability
- line: Approximate line number (or -1 if unclear)
- swc_id: Relevant SWC ID if applicable
- exploit_scenario: How an attacker could exploit this
- defi_impact: Potential financial impact on users/protocol
- mitigation: How to fix this issue

Focus on DeFi-specific issues and real-world exploit scenarios.
"""

            # Get OpenAI API key - check multiple sources for robustness
            import os
            api_key = os.getenv("OPENAI_API_KEY")  # First check environment variable
            
            if not api_key:
                # Fall back to config manager
                api_key = self.config.config.openai_api_key
            
            if not api_key:
                raise Exception("OpenAI API key not found in environment (OPENAI_API_KEY) or config file (~/.aether/config.yaml)")

            # Call GPT-5-pro for better DeFi analysis
            import openai
            client = openai.OpenAI(api_key=api_key)

            try:
                response = client.chat.completions.create(
                    model=_get_analysis_model(),
                    messages=[
                        {"role": "system", "content": self._get_persona_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    max_completion_tokens=4000
                )
            except Exception as e:
                # Fallback to gpt-4o if gpt-5-pro fails
                if 'model_not_found' in str(e).lower() or 'not available' in str(e).lower():
                    response = client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {"role": "system", "content": self._get_persona_prompt()},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=2000
                    )
                else:
                    raise

            # Parse response
            response_text = response.choices[0].message.content
            findings = self._parse_defi_findings(response_text)

            processing_time = time.time() - start_time

            result = ModelResult(
                model_name=self.agent_name,
                findings=findings,
                confidence=0.85,
                processing_time=processing_time,
                metadata={
                    "role": self.role,
                    "focus_areas": self.focus_areas,
                    "learning_patterns_applied": len(learning_context.get('learned_patterns', [])),
                    "contract_hash": contract_hash
                }
            )

            # Store result for learning
            self._store_analysis_result(result, contract_hash)

            return result

        except Exception as e:
            logger.error(f"DeFi Expert failed: {e}")
            processing_time = time.time() - start_time

            return ModelResult(
                model_name=self.agent_name,
                findings=[],
                confidence=0.0,
                processing_time=processing_time,
                metadata={"error": str(e), "role": self.role}
            )

    def _parse_defi_findings(self, response: str) -> List[Dict[str, Any]]:
        """Parse DeFi-specific findings from LLM response"""
        try:
            # Use the robust parse_llm_json utility instead of naive regex
            data = parse_llm_json(response, fallback={"findings": []})
            findings = data.get('findings', [])
            if isinstance(findings, list):
                return findings
            return []
        except Exception as e:
            logger.error(f"Failed to parse DeFi findings: {e}")
            return []


class GasOptimizationExpert(BaseAIModel):
    """Gas Optimization Expert - Focuses on gas efficiency and optimization opportunities"""

    def __init__(self):
        super().__init__(
            agent_name="gas_expert",
            role="Gas Optimization Expert",
            focus_areas=["gas_optimization", "efficiency", "storage", "computation"]
        )

    def _get_persona_prompt(self) -> str:
        return """You are a **Gas Optimization Expert** specializing in Ethereum gas efficiency and smart contract optimization.

**Your Mission:** Identify gas inefficiencies and optimization opportunities that can reduce transaction costs and improve contract performance.

**Focus Areas:**
- **Storage Optimization**: Expensive SLOAD/SSTORE operations, struct packing
- **Computation Efficiency**: Loop optimization, unnecessary calculations
- **Function Optimization**: Batch operations, view vs pure functions
- **Memory Management**: Memory vs storage usage, calldata optimization
- **Event Optimization**: Event emission efficiency

**Analysis Approach:**
1. **Gas Cost Analysis**: Calculate potential gas savings for each optimization
2. **Trade-off Evaluation**: Balance gas savings vs code complexity/readability
3. **Upgrade Compatibility**: Ensure optimizations don't break existing functionality
4. **Batch Processing**: Identify opportunities for batching operations

**Output Format:** JSON array of gas optimization opportunities"""

    async def analyze_contract(self, contract_content: str, contract_path: str = "", context: Dict[str, Any] = None) -> ModelResult:
        """Analyze contract for gas optimization opportunities"""
        start_time = time.time()

        try:
            learning_context = self._apply_learning_patterns(contract_content)
            contract_hash = hashlib.md5(contract_content.encode()).hexdigest()

            prompt = f"""
{self._get_persona_prompt()}

**CONTRACT TO ANALYZE:**
```solidity
{contract_content}
```

**LEARNING CONTEXT:**
{json.dumps(learning_context, indent=2)}

**REQUIRED OUTPUT:**
Return a JSON array of gas optimization opportunities. Each optimization should include:
- type: "gas_optimization"
- severity: "low" | "medium" | "high" (based on potential savings)
- confidence: 0.0-1.0
- description: What can be optimized
- line: Line number where optimization applies
- gas_savings_estimate: Estimated gas savings per call
- implementation_complexity: "low" | "medium" | "high"
- optimization_details: Technical details of the optimization
- code_suggestion: Suggested code changes

Focus on high-impact optimizations that provide significant gas savings.
"""

            # Get OpenAI API key - check multiple sources for robustness
            import os
            api_key = os.getenv("OPENAI_API_KEY")  # First check environment variable
            
            if not api_key:
                # Fall back to config manager
                api_key = self.config.config.openai_api_key
            
            if not api_key:
                raise Exception("OpenAI API key not found in environment (OPENAI_API_KEY) or config file (~/.aether/config.yaml)")

            import openai
            client = openai.OpenAI(api_key=api_key)

            try:
                response = client.chat.completions.create(
                    model=_get_analysis_model(),
                    messages=[
                        {"role": "system", "content": self._get_persona_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                )
            except Exception as e:
                # Fallback to gpt-4o if gpt-5-pro fails
                if 'model_not_found' in str(e).lower() or 'not available' in str(e).lower():
                    response = client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {"role": "system", "content": self._get_persona_prompt()},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=2000
                    )
                else:
                    raise

            response_text = response.choices[0].message.content
            findings = self._parse_gas_findings(response_text)

            processing_time = time.time() - start_time

            result = ModelResult(
                model_name=self.agent_name,
                findings=findings,
                confidence=0.85,
                processing_time=processing_time,
                metadata={
                    "role": self.role,
                    "focus_areas": self.focus_areas,
                    "learning_patterns_applied": len(learning_context.get('learned_patterns', [])),
                    "contract_hash": contract_hash
                }
            )

            self._store_analysis_result(result, contract_hash)
            return result

        except Exception as e:
            logger.error(f"Gas Expert failed: {e}")
            processing_time = time.time() - start_time

            return ModelResult(
                model_name=self.agent_name,
                findings=[],
                confidence=0.0,
                processing_time=processing_time,
                metadata={"error": str(e), "role": self.role}
            )

    def _parse_gas_findings(self, response: str) -> List[Dict[str, Any]]:
        """Parse gas optimization findings from LLM response"""
        try:
            # Use the robust parse_llm_json utility instead of naive regex
            data = parse_llm_json(response, fallback=[])
            if isinstance(data, list):
                return data
            findings = data.get('findings', []) if isinstance(data, dict) else []
            if isinstance(findings, list):
                return findings
            return []
        except Exception as e:
            logger.error(f"Failed to parse gas findings: {e}")
            return []


class SecurityBestPracticesExpert(BaseAIModel):
    """Security Best Practices Expert - Focuses on code quality and security standards"""

    def __init__(self):
        super().__init__(
            agent_name="best_practices_expert",
            role="Security Best Practices Expert",
            focus_areas=["code_quality", "security_standards", "access_control", "input_validation"]
        )

    def _get_persona_prompt(self) -> str:
        return """You are a **Security Best Practices Expert** focused on code quality, security standards, and development best practices.

**Your Mission:** Identify code quality issues, security anti-patterns, and deviations from established best practices.

**Focus Areas:**
- **Access Control**: Proper authorization, role-based access, ownership patterns
- **Input Validation**: Parameter validation, bounds checking, sanitization
- **Error Handling**: Proper error handling, fail-safe defaults, graceful degradation
- **Code Quality**: Readability, maintainability, documentation, testing
- **Security Standards**: Compliance with security best practices and standards

**Analysis Approach:**
1. **Standards Compliance**: Check against established security standards (SWC, best practices)
2. **Code Review**: Analyze code structure, patterns, and maintainability
3. **Security Anti-patterns**: Identify common security mistakes and bad practices
4. **Documentation**: Check for proper documentation and comments

**Output Format:** JSON array of best practice violations and recommendations"""

    async def analyze_contract(self, contract_content: str, contract_path: str = "", context: Dict[str, Any] = None) -> ModelResult:
        """Analyze contract for security best practices"""
        start_time = time.time()

        try:
            learning_context = self._apply_learning_patterns(contract_content)
            contract_hash = hashlib.md5(contract_content.encode()).hexdigest()

            prompt = f"""
{self._get_persona_prompt()}

**CONTRACT TO ANALYZE:**
```solidity
{contract_content}
```

**LEARNING CONTEXT:**
{json.dumps(learning_context, indent=2)}

**REQUIRED OUTPUT:**
Return a JSON array of best practice issues. Each finding should include:
- type: "best_practice_violation"
- severity: "low" | "medium" | "high" | "info"
- confidence: 0.0-1.0
- description: What best practice is being violated
- line: Line number where issue occurs
- standard: Relevant standard (SWC-XXX, best practice name)
- impact: Why this matters for security/maintainability
- recommendation: How to fix or improve

Focus on actionable improvements that enhance security and maintainability.
"""

            # Get OpenAI API key - check multiple sources for robustness
            import os
            api_key = os.getenv("OPENAI_API_KEY")  # First check environment variable
            
            if not api_key:
                # Fall back to config manager
                api_key = self.config.config.openai_api_key
            
            if not api_key:
                raise Exception("OpenAI API key not found in environment (OPENAI_API_KEY) or config file (~/.aether/config.yaml)")

            import openai
            client = openai.OpenAI(api_key=api_key)

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": self._get_persona_prompt()},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000
                # Removed temperature - using default value for compatibility
            )

            response_text = response.choices[0].message.content
            findings = self._parse_best_practices_findings(response_text)

            processing_time = time.time() - start_time

            result = ModelResult(
                model_name=self.agent_name,
                findings=findings,
                confidence=0.85,
                processing_time=processing_time,
                metadata={
                    "role": self.role,
                    "focus_areas": self.focus_areas,
                    "learning_patterns_applied": len(learning_context.get('learned_patterns', [])),
                    "contract_hash": contract_hash
                }
            )

            self._store_analysis_result(result, contract_hash)
            return result

        except Exception as e:
            logger.error(f"Best Practices Expert failed: {e}")
            processing_time = time.time() - start_time

            return ModelResult(
                model_name=self.agent_name,
                findings=[],
                confidence=0.0,
                processing_time=processing_time,
                metadata={"error": str(e), "role": self.role}
            )

    def _parse_best_practices_findings(self, response: str) -> List[Dict[str, Any]]:
        """Parse best practices findings from LLM response"""
        try:
            # Use the robust parse_llm_json utility instead of naive regex
            data = parse_llm_json(response, fallback=[])
            if isinstance(data, list):
                return data
            findings = data.get('findings', []) if isinstance(data, dict) else []
            if isinstance(findings, list):
                return findings
            return []
        except Exception as e:
            logger.error(f"Failed to parse best practices findings: {e}")
            return []


# Phase 3 test-friendly analyzer stubs expected by tests
class GPT5SecurityAuditor(BaseAIModel):
    def __init__(self):
        super().__init__(agent_name='gpt5_security', role='Security Vulnerability Auditor', focus_areas=['access_control', 'reentrancy', 'overflow', 'external_calls'])

    async def analyze_contract(self, contract_content: str, contract_path: str = "", context: Dict[str, Any] = None) -> ModelResult:
        start = time.time()

        try:
            # Get configuration
            from core.config_manager import ConfigManager
            config = ConfigManager()
            api_key = getattr(config.config, 'openai_api_key', None)
            # Get agent-specific model from config, fallback to analysis model
            model = getattr(config.config, 'agent_gpt5_security_model', None) or _get_analysis_model()

            if not api_key:
                return ModelResult(
                    model_name='gpt5_security',
                    findings=[],
                    confidence=0.0,
                    processing_time=time.time() - start,
                    metadata={'persona': 'security_auditor', 'error': 'No OpenAI API key configured'}
                )

            # Create OpenAI API prompt for security auditing
            prompt = f"""
You are a senior smart contract security auditor specializing in access control, reentrancy, overflow, and external calls.

Focus your analysis on these specific vulnerability types:
- Access control issues (missing onlyOwner, role-based access)
- Reentrancy vulnerabilities (cross-function reentrancy, read-only reentrancy)
- Integer overflow/underflow in arithmetic operations
- Unsafe external calls and delegatecall usage

Contract to analyze:
```solidity
{contract_content[:16000]}  # Use larger context with gpt-5-pro
```

Please analyze this contract and return findings in the exact JSON format:
{{
    "findings": [
        {{
            "type": "vulnerability_type",
            "severity": "critical|high|medium|low",
            "confidence": 0.0-1.0,
            "description": "detailed explanation",
            "line": line_number,
            "swc_id": "SWC-XXX"
        }}
    ]
}}

Return only valid JSON, no markdown formatting.
"""

            # Make OpenAI API call
            import openai
            import json

            client = openai.OpenAI(api_key=api_key)

            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "You are an expert smart contract security auditor."},
                        {"role": "user", "content": prompt}
                    ]
                )
            except Exception as e:
                # Fallback to gpt-4o if gpt-5-mini fails
                if 'model_not_found' in str(e).lower() or 'not available' in str(e).lower():
                    model = 'gpt-4o'
                    response = client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": "You are an expert smart contract security auditor."},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=2000
                    )
                else:
                    raise

            response_text = response.choices[0].message.content

            # Parse JSON response robustly
            findings_data = parse_llm_json(response_text, fallback={"findings": []})
            findings = findings_data.get('findings', [])

            return ModelResult(
                model_name='gpt5_security',
                findings=findings,
                confidence=0.85,
                processing_time=time.time() - start,
                metadata={'persona': 'security_auditor', 'api_calls': 1, 'model': model}
            )

        except Exception as e:
            logger.error(f"Security Auditor failed: {e}")
            return ModelResult(
                model_name='gpt5_security',
                findings=[],
                confidence=0.0,
                processing_time=time.time() - start,
                metadata={'persona': 'security_auditor', 'error': str(e)}
            )


class GPT5DeFiSpecialist(BaseAIModel):
    def __init__(self):
        super().__init__(agent_name='gpt5_defi', role='DeFi Protocol Specialist', focus_areas=['amm', 'lending', 'governance', 'oracle_manipulation'])

    async def analyze_contract(self, contract_content: str, contract_path: str = "", context: Dict[str, Any] = None) -> ModelResult:
        start = time.time()

        try:
            # Get configuration
            from core.config_manager import ConfigManager
            config = ConfigManager()
            api_key = getattr(config.config, 'openai_api_key', None)
            # Get agent-specific model from config, fallback to analysis model
            model = getattr(config.config, 'agent_gpt5_defi_model', None) or _get_analysis_model()

            if not api_key:
                return ModelResult(
                    model_name='gpt5_defi',
                    findings=[],
                    confidence=0.0,
                    processing_time=time.time() - start,
                    metadata={'persona': 'defi_specialist', 'error': 'No OpenAI API key configured'}
                )

            # Create OpenAI API prompt for DeFi specialist
            prompt = f"""
You are a senior DeFi protocol specialist analyzing smart contracts for AMM, lending, governance, and oracle manipulation vulnerabilities.

Focus your analysis on these specific vulnerability types:
- AMM price manipulation and flash loan attacks
- Lending protocol liquidation vulnerabilities
- Governance proposal manipulation and voting issues
- Oracle price feed manipulation and stale data

Contract to analyze:
```solidity
{contract_content[:16000]}  # Use larger context with gpt-5-pro
```

Please analyze this contract and return findings in the exact JSON format:
{{
    "findings": [
        {{
            "type": "vulnerability_type",
            "severity": "critical|high|medium|low",
            "confidence": 0.0-1.0,
            "description": "detailed explanation",
            "line": line_number,
            "swc_id": "SWC-XXX"
        }}
    ]
}}

Return only valid JSON, no markdown formatting.
"""

            # Make OpenAI API call
            import openai
            import json

            client = openai.OpenAI(api_key=api_key)

            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "You are an expert DeFi protocol security specialist."},
                        {"role": "user", "content": prompt}
                    ]
                )
            except Exception as e:
                # Fallback to gpt-4o if gpt-5-mini fails
                if 'model_not_found' in str(e).lower() or 'not available' in str(e).lower():
                    model = 'gpt-4o'
                    response = client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": "You are an expert DeFi protocol security specialist."},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=2000
                    )
                else:
                    raise

            response_text = response.choices[0].message.content

            # Parse JSON response robustly
            findings_data = parse_llm_json(response_text, fallback={"findings": []})
            findings = findings_data.get('findings', [])

            return ModelResult(
                model_name='gpt5_defi',
                findings=findings,
                confidence=0.8,
                processing_time=time.time() - start,
                metadata={'persona': 'defi_specialist', 'api_calls': 1, 'model': model}
            )

        except Exception as e:
            logger.error(f"DeFi Specialist failed: {e}")
            return ModelResult(
                model_name='gpt5_defi',
                findings=[],
                confidence=0.0,
                processing_time=time.time() - start,
                metadata={'persona': 'defi_specialist', 'error': str(e)}
            )


class GeminiSecurityAuditor(BaseAIModel):
    def __init__(self):
        super().__init__(agent_name='gemini_security', role='Gemini Security Vulnerability Hunter', focus_areas=['external_calls', 'delegatecall', 'tx_origin', 'unchecked_returns'])

    async def analyze_contract(self, contract_content: str, contract_path: str = "", context: Dict[str, Any] = None) -> ModelResult:
        start = time.time()

        try:
            # Get configuration
            from core.config_manager import ConfigManager
            config = ConfigManager()
            api_key = getattr(config.config, 'gemini_api_key', None)
            # Get agent-specific model from config, fallback to gemini-2.5-flash
            model = getattr(config.config, 'agent_gemini_security_model', 'gemini-2.5-flash')

            if not api_key:
                return ModelResult(
                    model_name='gemini_security',
                    findings=[],
                    confidence=0.0,
                    processing_time=time.time() - start,
                    metadata={'persona': 'gemini_security_hunter', 'error': 'No Gemini API key configured'}
                )

            # Create Gemini API prompt for security auditing
            prompt = f"""You are an expert smart contract security auditor performing automated code review.

Task: Analyze the Solidity smart contract below for security vulnerabilities.

Analysis Focus Areas:
- External call safety and reentrancy patterns
- Delegatecall usage and context preservation
- tx.origin authentication vulnerabilities
- Unchecked return values from external calls
- Access control mechanisms
- State management and race conditions

Contract Code:
```solidity
{contract_content[:8000]}
```

Output Requirements:
1. Return valid JSON only (no markdown formatting or code blocks)
2. Use this exact structure: {{"findings": [...]}}
3. If no vulnerabilities are found, return: {{"findings": []}}
4. Each finding must include these fields:
   - type: vulnerability category (string)
   - severity: "critical", "high", "medium", or "low"
   - confidence: numeric value between 0.0 and 1.0
   - description: detailed explanation of the issue
   - line: approximate line number (integer)
   - swc_id: relevant SWC identifier (e.g., "SWC-107")

Expected JSON format:
{{"findings": [{{"type": "...", "severity": "...", "confidence": 0.0, "description": "...", "line": 0, "swc_id": "..."}}]}}

Note: This is automated security analysis for code review purposes, conducted by authorized developers
as part of standard software quality assurance and security testing before deployment."""

            # Make Gemini API call
            import requests
            import json

            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"

            payload = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "maxOutputTokens": 100000,
                },
                "safetySettings": [
                    {
                        "category": "HARM_CATEGORY_HATE_SPEECH",
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_HARASSMENT",
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                        "threshold": "BLOCK_NONE"
                    }
                ]
            }

            # Gemini API can be slow, use longer timeout (60 seconds) with retry logic
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    response = requests.post(url, json=payload, timeout=60)
                    response.raise_for_status()
                    break  # Success, exit retry loop
                except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                    if attempt < max_retries - 1:
                        logger.warning(f"[Gemini Security] Timeout on attempt {attempt + 1}/{max_retries}, retrying...")
                        time.sleep(2)  # Wait before retry (use module-level time import)
                    else:
                        logger.error(f"[Gemini Security] Timeout after {max_retries} attempts: {e}")
                        return ModelResult(
                            model_name='gemini_security',
                            findings=[],
                            confidence=0.0,
                            processing_time=time.time() - start,
                            metadata={'persona': 'gemini_security_hunter', 'error': f'API timeout: {str(e)}'}
                        )

            result = response.json()
            findings = []
            try:
                candidates = result.get('candidates') or []
                if candidates:
                    candidate = candidates[0]
                    
                    # Check if response was blocked mid-generation by safety filters
                    finish_reason = candidate.get('finishReason', '')
                    if finish_reason == 'SAFETY':
                        logger.error(f"[Gemini Security] Content blocked by safety filters during generation")
                        logger.error(f"[Gemini Security] finishReason: {finish_reason}")
                        if 'safetyRatings' in candidate:
                            logger.error(f"[Gemini Security] Safety Ratings: {json.dumps(candidate['safetyRatings'], indent=2)}")
                        # Return empty findings with clear error
                        return ModelResult(
                            model_name='gemini_security',
                            findings=[],
                            confidence=0.0,
                            processing_time=time.time() - start,
                            metadata={
                                'persona': 'gemini_security_hunter',
                                'error': 'Content blocked by Gemini safety filters (finishReason: SAFETY)',
                                'finish_reason': finish_reason
                            }
                        )
                    
                    # Handle MAX_TOKENS finish reason
                    if finish_reason == 'MAX_TOKENS':
                        logger.warning(f"[Gemini Security] Response truncated by MAX_TOKENS limit. Retrying with condensed prompt...")
                        # Create a shorter, more focused prompt
                        condensed_prompt = f"""Analyze for CRITICAL security vulnerabilities only:
```solidity
{contract_content[:4000]}
```
Return JSON: {{"findings": [{{"type": "...", "severity": "critical|high", "confidence": 0.0, "description": "...", "line": 0, "swc_id": "..."}}]}}
Return only JSON, no markdown."""
                        
                        try:
                            retry_payload = {
                                "contents": [{
                                    "parts": [{
                                        "text": condensed_prompt
                                    }]
                                }],
                                "generationConfig": {
                                    "maxOutputTokens": 1500,
                                },
                                "safetySettings": payload["safetySettings"]
                            }
                            
                            retry_response = requests.post(url, json=retry_payload, timeout=60)
                            retry_response.raise_for_status()
                            result = retry_response.json()
                            candidates = result.get('candidates') or []
                            if not candidates:
                                logger.error(f"[Gemini Security] Retry also failed - no candidates")
                                return ModelResult(
                                    model_name='gemini_security',
                                    findings=[],
                                    confidence=0.0,
                                    processing_time=time.time() - start,
                                    metadata={
                                        'persona': 'gemini_security_hunter',
                                        'error': 'MAX_TOKENS: Gemini cannot process this contract - response too long',
                                        'finish_reason': 'MAX_TOKENS'
                                    }
                                )
                            candidate = candidates[0]
                            finish_reason = candidate.get('finishReason', '')
                        except Exception as retry_error:
                            logger.error(f"[Gemini Security] Retry failed: {retry_error}")
                            return ModelResult(
                                model_name='gemini_security',
                                findings=[],
                                confidence=0.0,
                                processing_time=time.time() - start,
                                metadata={
                                    'persona': 'gemini_security_hunter',
                                    'error': f'MAX_TOKENS: Retry failed - {str(retry_error)}',
                                    'finish_reason': 'MAX_TOKENS'
                                }
                            )
                    
                    content = candidate.get('content') or {}
                    parts = content.get('parts') or []
                    
                    # Debug the structure
                    logger.debug(f"[Gemini Security] Response structure - candidates: {len(candidates)}, parts: {len(parts)}, finishReason: {finish_reason}")
                    
                    if parts and isinstance(parts, list):
                        # Find text in parts (could be at any index)
                        response_text = None
                        for i, part in enumerate(parts):
                            if isinstance(part, dict) and 'text' in part:
                                response_text = part['text']
                                logger.debug(f"[Gemini Security] Found text in part {i}")
                                break
                        
                        if response_text:
                            logger.debug(f"[Gemini Security] Raw response (first 500 chars): {response_text[:500]}")
                            data = parse_llm_json(response_text, fallback={"findings": []})
                            findings = data.get('findings', [])
                            logger.debug(f"[Gemini Security] Parsed {len(findings)} findings")
                        else:
                            logger.warning(f"[Gemini Security] No text found in any part. Parts: {[p.keys() if isinstance(p, dict) else type(p) for p in parts]}")
                    else:
                        # Empty parts - check if this is due to safety filtering
                        if finish_reason:
                            logger.warning(f"[Gemini Security] Empty parts array with finishReason: {finish_reason}")
                        else:
                            logger.warning(f"[Gemini Security] Parts is not a list or is empty: {type(parts)} - {parts}")
                else:
                    logger.warning(f"[Gemini Security] No candidates in response")
                    if 'promptFeedback' in result:
                        feedback = result['promptFeedback']
                        logger.error(f"[Gemini Security] SAFETY FILTER BLOCKED (pre-generation): {json.dumps(feedback, indent=2)}")
                        if 'blockReason' in feedback:
                            logger.error(f"[Gemini Security] Block Reason: {feedback['blockReason']}")
                        if 'safetyRatings' in feedback:
                            logger.error(f"[Gemini Security] Safety Ratings: {json.dumps(feedback['safetyRatings'], indent=2)}")
            except Exception as e:
                logger.error(f"[Gemini Security] Error parsing findings: {e}")
                logger.debug(f"[Gemini Security] Response was: {result}")
                import traceback
                logger.debug(f"[Gemini Security] Traceback: {traceback.format_exc()}")
                findings = []

            return ModelResult(
                model_name='gemini_security',
                findings=findings,
                confidence=0.8,
                processing_time=time.time() - start,
                metadata={'persona': 'gemini_security_hunter', 'api_calls': 1, 'model': 'gemini-2.5-flash'}
            )

        except Exception as e:
            logger.error(f"Gemini Security Auditor failed: {e}")
            return ModelResult(
                model_name='gemini_security',
                findings=[],
                confidence=0.0,
                processing_time=time.time() - start,
                metadata={'persona': 'gemini_security_hunter', 'error': str(e)}
            )


class GeminiFormalVerifier(BaseAIModel):
    def __init__(self):
        super().__init__(agent_name='gemini_verification', role='Gemini Formal Verification Specialist', focus_areas=['arithmetic', 'overflow', 'underflow', 'precision_loss'])

    async def analyze_contract(self, contract_content: str, contract_path: str = "", context: Dict[str, Any] = None) -> ModelResult:
        start = time.time()

        try:
            # Get configuration
            from core.config_manager import ConfigManager
            config = ConfigManager()
            api_key = getattr(config.config, 'gemini_api_key', None)
            # Get agent-specific model from config, fallback to gemini-2.5-pro for formal verification
            model = getattr(config.config, 'agent_gemini_verification_model', 'gemini-2.5-pro')

            if not api_key:
                return ModelResult(
                    model_name='gemini_verification',
                    findings=[],
                    confidence=0.0,
                    processing_time=time.time() - start,
                    metadata={'persona': 'gemini_formal_verifier', 'error': 'No Gemini API key configured'}
                )

            # Create Gemini API prompt for formal verification
            prompt = f"""You are an expert formal verification specialist for smart contract arithmetic analysis.

Task: Analyze the Solidity smart contract below for mathematical and arithmetic vulnerabilities.

Analysis Focus Areas:
- Integer overflow and underflow conditions
- Precision loss in mathematical operations
- Unsafe type casting between numeric types
- Division by zero vulnerabilities
- Mathematical invariant violations
- Rounding errors in financial calculations

Contract Code:
```solidity
{contract_content[:8000]}
```

Output Requirements:
1. Return valid JSON only (no markdown formatting or code blocks)
2. Use this exact structure: {{"findings": [...]}}
3. If no vulnerabilities are found, return: {{"findings": []}}
4. Each finding must include these fields:
   - type: vulnerability category (string)
   - severity: "critical", "high", "medium", or "low"
   - confidence: numeric value between 0.0 and 1.0
   - description: detailed explanation of the issue
   - line: approximate line number (integer)
   - swc_id: relevant SWC identifier (e.g., "SWC-101")

Expected JSON format:
{{"findings": [{{"type": "...", "severity": "...", "confidence": 0.0, "description": "...", "line": 0, "swc_id": "..."}}]}}

Note: This is automated arithmetic verification for code review purposes, conducted by authorized developers
as part of standard software quality assurance and security testing before deployment."""

            # Make Gemini API call
            import requests
            import json

            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"

            payload = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "maxOutputTokens": 100000,
                },
                "safetySettings": [
                    {
                        "category": "HARM_CATEGORY_HATE_SPEECH",
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_HARASSMENT",
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                        "threshold": "BLOCK_NONE"
                    }
                ]
            }

            # Gemini API can be slow, use longer timeout (60 seconds) with retry logic
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    response = requests.post(url, json=payload, timeout=60)
                    response.raise_for_status()
                    break  # Success, exit retry loop
                except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                    if attempt < max_retries - 1:
                        logger.warning(f"[Gemini Verifier] Timeout on attempt {attempt + 1}/{max_retries}, retrying...")
                        time.sleep(2)  # Wait before retry (use module-level time import)
                    else:
                        logger.error(f"[Gemini Verifier] Timeout after {max_retries} attempts: {e}")
                        return ModelResult(
                            model_name='gemini_verification',
                            findings=[],
                            confidence=0.0,
                            processing_time=time.time() - start,
                            metadata={'persona': 'gemini_formal_verifier', 'error': f'API timeout: {str(e)}'}
                        )

            result = response.json()
            findings = []
            try:
                candidates = result.get('candidates') or []
                if candidates:
                    candidate = candidates[0]
                    
                    # Check if response was blocked mid-generation by safety filters
                    finish_reason = candidate.get('finishReason', '')
                    if finish_reason == 'SAFETY':
                        logger.error(f"[Gemini Verifier] Content blocked by safety filters during generation")
                        logger.error(f"[Gemini Verifier] finishReason: {finish_reason}")
                        if 'safetyRatings' in candidate:
                            logger.error(f"[Gemini Verifier] Safety Ratings: {json.dumps(candidate['safetyRatings'], indent=2)}")
                        # Return empty findings with clear error
                        return ModelResult(
                            model_name='gemini_verification',
                            findings=[],
                            confidence=0.0,
                            processing_time=time.time() - start,
                            metadata={
                                'persona': 'gemini_formal_verifier',
                                'error': 'Content blocked by Gemini safety filters (finishReason: SAFETY)',
                                'finish_reason': finish_reason
                            }
                        )
                    
                    # Handle MAX_TOKENS finish reason
                    if finish_reason == 'MAX_TOKENS':
                        logger.warning(f"[Gemini Verifier] Response truncated by MAX_TOKENS limit. Retrying with condensed prompt...")
                        # Create a shorter, more focused prompt
                        condensed_prompt = f"""Analyze for CRITICAL arithmetic/overflow vulnerabilities only:
```solidity
{contract_content[:4000]}
```
Return JSON: {{"findings": [{{"type": "...", "severity": "critical|high", "confidence": 0.0, "description": "...", "line": 0, "swc_id": "..."}}]}}
Return only JSON, no markdown."""
                        
                        try:
                            retry_payload = {
                                "contents": [{
                                    "parts": [{
                                        "text": condensed_prompt
                                    }]
                                }],
                                "generationConfig": {
                                    "maxOutputTokens": 1200,
                                },
                                "safetySettings": payload["safetySettings"]
                            }
                            
                            retry_response = requests.post(url, json=retry_payload, timeout=60)
                            retry_response.raise_for_status()
                            result = retry_response.json()
                            candidates = result.get('candidates') or []
                            if not candidates:
                                logger.error(f"[Gemini Verifier] Retry also failed - no candidates")
                                return ModelResult(
                                    model_name='gemini_verification',
                                    findings=[],
                                    confidence=0.0,
                                    processing_time=time.time() - start,
                                    metadata={
                                        'persona': 'gemini_formal_verifier',
                                        'error': 'MAX_TOKENS: Gemini cannot process this contract - response too long',
                                        'finish_reason': 'MAX_TOKENS'
                                    }
                                )
                            candidate = candidates[0]
                            finish_reason = candidate.get('finishReason', '')
                        except Exception as retry_error:
                            logger.error(f"[Gemini Verifier] Retry failed: {retry_error}")
                            return ModelResult(
                                model_name='gemini_verification',
                                findings=[],
                                confidence=0.0,
                                processing_time=time.time() - start,
                                metadata={
                                    'persona': 'gemini_formal_verifier',
                                    'error': f'MAX_TOKENS: Retry failed - {str(retry_error)}',
                                    'finish_reason': 'MAX_TOKENS'
                                }
                            )
                    
                    content = candidate.get('content') or {}
                    parts = content.get('parts') or []
                    
                    # Debug the structure
                    logger.debug(f"[Gemini Verifier] Response structure - candidates: {len(candidates)}, parts: {len(parts)}, finishReason: {finish_reason}")
                    
                    if parts and isinstance(parts, list):
                        # Find text in parts (could be at any index)
                        response_text = None
                        for i, part in enumerate(parts):
                            if isinstance(part, dict) and 'text' in part:
                                response_text = part['text']
                                logger.debug(f"[Gemini Verifier] Found text in part {i}")
                                break
                        
                        if response_text:
                            logger.debug(f"[Gemini Verifier] Raw response (first 500 chars): {response_text[:500]}")
                            data = parse_llm_json(response_text, fallback={"findings": []})
                            findings = data.get('findings', [])
                            logger.debug(f"[Gemini Verifier] Parsed {len(findings)} findings")
                        else:
                            logger.warning(f"[Gemini Verifier] No text found in any part. Parts: {[p.keys() if isinstance(p, dict) else type(p) for p in parts]}")
                    else:
                        # Empty parts - check if this is due to safety filtering
                        if finish_reason:
                            logger.warning(f"[Gemini Verifier] Empty parts array with finishReason: {finish_reason}")
                        else:
                            logger.warning(f"[Gemini Verifier] Parts is not a list or is empty: {type(parts)} - {parts}")
                else:
                    logger.warning(f"[Gemini Verifier] No candidates in response")
                    if 'promptFeedback' in result:
                        feedback = result['promptFeedback']
                        logger.error(f"[Gemini Verifier] SAFETY FILTER BLOCKED (pre-generation): {json.dumps(feedback, indent=2)}")
                        if 'blockReason' in feedback:
                            logger.error(f"[Gemini Verifier] Block Reason: {feedback['blockReason']}")
                        if 'safetyRatings' in feedback:
                            logger.error(f"[Gemini Verifier] Safety Ratings: {json.dumps(feedback['safetyRatings'], indent=2)}")
            except Exception as e:
                logger.error(f"[Gemini Verifier] Error parsing findings: {e}")
                logger.debug(f"[Gemini Verifier] Response was: {result}")
                import traceback
                logger.debug(f"[Gemini Verifier] Traceback: {traceback.format_exc()}")
                findings = []

            return ModelResult(
                model_name='gemini_verification',
                findings=findings,
                confidence=0.9,
                processing_time=time.time() - start,
                metadata={'persona': 'gemini_formal_verifier', 'api_calls': 1}
            )

        except Exception as e:
            logger.error(f"Gemini Formal Verifier failed: {e}")
            return ModelResult(
                model_name='gemini_verification',
                findings=[],
                confidence=0.0,
                processing_time=time.time() - start,
                metadata={'persona': 'gemini_formal_verifier', 'error': str(e)}
            )


class DeFiSpecialistModel(BaseAIModel):
    def __init__(self):
        super().__init__(agent_name='defi_specialist', role='DeFi Specialist', focus_areas=['defi'])

    async def analyze_contract(self, contract_content: str, contract_path: str = "", context: Dict[str, Any] = None) -> ModelResult:
        start = time.time()
        findings = [{
            'type': 'reentrancy',
            'severity': 'high',
            'confidence': 0.7 * self.confidence_weight,
            'description': 'DeFi specific reentrancy risk',
            'line': 10,
        }]
        return ModelResult(model_name=self.agent_name, findings=findings, confidence=0.7, processing_time=time.time() - start, metadata={})


class FormalVerificationModel(BaseAIModel):
    def __init__(self):
        super().__init__(agent_name='formal_verification', role='Formal Verification', focus_areas=['formal'])

    async def analyze_contract(self, contract_content: str, contract_path: str = "", context: Dict[str, Any] = None) -> ModelResult:
        start = time.time()
        findings = [{
            'type': 'reentrancy',
            'severity': 'high',
            'confidence': 0.9 * self.confidence_weight,
            'description': 'Invariant suggests reentrancy possibility',
            'line': 10,
        }]
        return ModelResult(model_name=self.agent_name, findings=findings, confidence=0.9, processing_time=time.time() - start, metadata={})

class AIEnsemble:
    """Enhanced AI ensemble with specialized agents and database learning"""

    def __init__(self):
        self.db_manager = DatabaseManager()
        self.config = ConfigManager()

        # Three lightweight agents used by unit tests
        self.agents = [
            DeFiSecurityExpert(),
            GasOptimizationExpert(),
            SecurityBestPracticesExpert(),
        ]

        # Full model map retained for integration flows
        self.models = {
            'gpt5_security': GPT5SecurityAuditor(),
            'gpt5_defi': GPT5DeFiSpecialist(),
            'gemini_security': GeminiSecurityAuditor(),
            'gemini_verification': GeminiFormalVerifier(),
        }

        logger.info(f"Initialized AI Ensemble with {len(self.models)} models")

    async def analyze_with_ensemble(self, contract_content: str, contract_path: str = "") -> ConsensusResult:
        """Run all specialized agents and generate consensus results
        
        🔍 DEBUGGING INFO:
        This method now provides detailed insights into why model_agreement may be 0.0:
        
        1. **Agent Failures**: Shows which agents failed and why (usually missing OpenAI API key)
        2. **No Findings**: Shows when agents completed but found no vulnerabilities  
        3. **No Consensus**: Shows when agents found vulnerabilities but didn't agree (different types/severity)
        4. **Consensus Success**: Shows when 2+ agents agreed on the same finding
        
        ⚠️  COMMON ISSUE - Model Agreement = 0.0 Causes:
        - All agents fail (missing OPENAI_API_KEY) → all return empty findings
        - Agents find different vulnerabilities → no overlap between agents  
        - Agents are disabled or return empty results
        
        ✅ SOLUTION:
        - Set OPENAI_API_KEY environment variable: export OPENAI_API_KEY="your-key"
        - Or configure via: aether config --openai-key "your-key"
        - Check debug output for which agents are failing and why
        """
        start_time = time.time()

        try:
            # Prefer test-friendly agents when available; fallback to full models
            sources = self.agents if getattr(self, 'agents', None) else list(self.models.values())
            tasks = [m.analyze_contract(contract_content, contract_path) for m in sources]

            # Execute all agent analyses concurrently
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Process results and filter out exceptions
            valid_results = []
            for i, result in enumerate(results):
                agent_name = sources[i].agent_name if i < len(sources) else "unknown"
                
                if isinstance(result, Exception):
                    logger.error(f"❌ Agent {agent_name} failed with exception: {result}")
                    print(f"❌ AI Agent '{agent_name}' failed: {str(result)[:100]}")
                    continue

                if isinstance(result, ModelResult):
                    if result.findings:
                        valid_results.append(result)
                        logger.info(f"✅ Agent {agent_name} found {len(result.findings)} findings")
                        print(f"✅ AI Agent '{agent_name}' found {len(result.findings)} findings")
                    else:
                        if result.confidence == 0.0 and result.metadata.get('error'):
                            logger.warning(f"⚠️  Agent {agent_name} returned no findings - Error: {result.metadata.get('error')}")
                            print(f"⚠️  AI Agent '{agent_name}' failed: {result.metadata.get('error', 'Unknown error')[:100]}")
                        else:
                            logger.info(f"ℹ️  Agent {agent_name} completed but found no vulnerabilities")
                            print(f"ℹ️  AI Agent '{agent_name}' found no vulnerabilities in the contract")

            print(f"\n📊 AI Ensemble Summary:")
            print(f"   Total agents: {len(sources)}")
            print(f"   Successful agents: {len(valid_results)}")
            print(f"   Failed/No findings: {len(sources) - len(valid_results)}")

            # Generate consensus analysis
            consensus = self._generate_consensus(valid_results)

            processing_time = time.time() - start_time

            return ConsensusResult(
                consensus_findings=consensus['findings'],
                model_agreement=consensus['agreement'],
                confidence_score=consensus['confidence'],
                processing_time=processing_time,
                individual_results=valid_results
            )

        except Exception as e:
            logger.error(f"Enhanced ensemble analysis failed: {e}")
            print(f"❌ AI Ensemble analysis failed: {e}")
            processing_time = time.time() - start_time

            return ConsensusResult(
                consensus_findings=[],
                model_agreement=0.0,
                confidence_score=0.0,
                processing_time=processing_time,
                individual_results=[]
            )

    def _generate_consensus(self, results: List[ModelResult]) -> Dict[str, Any]:
        """Generate consensus from multiple agent results"""
        if not results:
            logger.warning("No valid AI agent results to generate consensus from")
            print("⚠️  No AI agents produced valid results - skipping consensus generation")
            return {
                'findings': [],
                'agreement': 0.0,
                'confidence': 0.0
            }

        # Collect all findings and track originating models
        all_findings = []
        all_findings_with_models = []  # List[Tuple[Dict, str]]
        findings_by_agent = {}
        for result in results:
            findings_by_agent[result.model_name] = len(result.findings)
            all_findings.extend(result.findings)
            for f in result.findings:
                all_findings_with_models.append((f, result.model_name))

        if not all_findings:
            logger.info("AI agents found no vulnerabilities to reach consensus on")
            print("ℹ️  AI agents analyzed contract but found no vulnerabilities to create consensus")
            return {
                'findings': [],
                'agreement': 0.0,
                'confidence': 0.0
            }

        # Log findings by each agent
        print(f"\n📋 Findings by Agent:")
        for agent_name, count in findings_by_agent.items():
            print(f"   - {agent_name}: {count} findings")

        # NEW APPROACH: Instead of strict consensus, deduplicate findings and pass to LLM for verification
        # This is better because different agents may describe the same vulnerability differently
        deduplicated_findings = self._deduplicate_findings(all_findings, all_findings_with_models)
        
        # Calculate agreement metrics
        total_agents = len(self.models)
        agreement_score = len(deduplicated_findings) / len(all_findings) if all_findings else 0.0
        
        # Weight by agent confidence
        avg_confidence = sum(r.confidence for r in results) / len(results) if results else 0.0
        
        if deduplicated_findings:
            print(f"✅ AI Ensemble: {len(deduplicated_findings)} deduplicated findings from {len(all_findings)} total")
            print(f"   Agents contributing: {', '.join(findings_by_agent.keys())}")
        else:
            print(f"ℹ️  AI Ensemble: All {len(all_findings)} findings appear to be duplicates")
        
        return {
            'findings': deduplicated_findings,
            'agreement': min(agreement_score, 1.0),
            'confidence': min(avg_confidence, 1.0),
            'all_findings': all_findings_with_models,  # Return all findings for downstream verification
            'agent_count': total_agents,
            'successful_agents': len([r for r in results if r.findings])
        }

    def _get_finding_key(self, finding: Dict[str, Any]) -> str:
        """Generate a key for finding similarity comparison"""
        # Use type, severity, and line number for similarity
        return f"{finding.get('type', '')}_{finding.get('severity', '')}_{finding.get('line', -1)}"

    def _merge_similar_findings(self, similar_findings: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Merge similar findings into a consensus finding"""
        # Take the finding with highest confidence as base
        base_finding = max(similar_findings, key=lambda f: f.get('confidence', 0))

        # Average confidence across similar findings
        avg_confidence = sum(f.get('confidence', 0) for f in similar_findings) / len(similar_findings)

        # Update with consensus confidence
        consensus_finding = base_finding.copy()
        consensus_finding['confidence'] = avg_confidence
        consensus_finding['consensus_count'] = len(similar_findings)
        consensus_finding['model_count'] = len(similar_findings)
        consensus_finding['consensus_confidence'] = min(1.0, avg_confidence)
        # Track which models agreed on this finding
        consensus_finding['models'] = []
        
        return consensus_finding

    def _deduplicate_findings(self, all_findings: List[Dict[str, Any]], all_findings_with_models: List[Tuple[Dict, str]]) -> List[Dict[str, Any]]:
        """
        Deduplicate findings using semantic similarity.
        This is a simplified approach and would require a more sophisticated LLM call
        to truly understand semantic similarity between findings.
        For now, we'll use a basic key-based approach and then merge.
        """
        # Create a dictionary to hold findings with their keys
        findings_by_key = {}
        for finding in all_findings:
            key = self._get_finding_key(finding)
            if key not in findings_by_key:
                findings_by_key[key] = finding
            else:
                # If a finding with the same key already exists, merge it
                existing_finding = findings_by_key[key]
                merged_finding = self._merge_similar_findings([existing_finding, finding])
                findings_by_key[key] = merged_finding

        # Convert back to a list and sort by confidence (descending)
        deduplicated_findings = sorted(list(findings_by_key.values()), key=lambda f: f.get('confidence', 0), reverse=True)
        return deduplicated_findings

    async def analyze_contract_ensemble(self, contract_content: str) -> ConsensusResult:
        """Legacy method name for backward compatibility with enhanced audit engine."""
        return await self.analyze_with_ensemble(contract_content)

    def get_learning_patterns(self) -> Dict[str, Any]:
        """Get current learning patterns from database"""
        try:
            # Get patterns for all agent focus areas
            all_patterns = []
            for agent in self.models.values():
                agent_patterns = []
                try:
                    # agent.focus_areas is a list; fetch per area
                    for area in getattr(agent, 'focus_areas', []) or []:
                        if hasattr(self.db_manager, 'get_learning_patterns'):
                            agent_patterns.extend(self.db_manager.get_learning_patterns(pattern_type=area) or [])
                except Exception:
                    agent_patterns = []
                all_patterns.extend(agent_patterns)

            return {
                'total_patterns': len(all_patterns),
                'high_confidence_patterns': len([p for p in all_patterns if (p.get('success_rate') if isinstance(p, dict) else getattr(p, 'success_rate', 0)) > 0.7]),
                'patterns_by_type': {}
            }
        except Exception as e:
            logger.error(f"Failed to get learning patterns: {e}")
            return {'total_patterns': 0, 'high_confidence_patterns': 0, 'patterns_by_type': {}}

    def get_model_specializations(self) -> Dict[str, List[str]]:
        return {
            'gpt5_security': ['access_control', 'reentrancy', 'overflow', 'external_calls'],
            'gpt5_defi': ['amm', 'lending', 'governance', 'oracle_manipulation'],
            'gemini_security': ['external_calls', 'delegatecall', 'tx_origin', 'unchecked_returns'],
            'gemini_verification': ['arithmetic', 'overflow', 'underflow', 'precision_loss'],
        }

    def update_model_weights(self, weights: Dict[str, float]) -> None:
        for key, w in weights.items():
            if key in self.models:
                self.models[key].confidence_weight = float(w)

    def get_ensemble_stats(self) -> Dict[str, Any]:
        return {
            'total_models': 4,
            'consensus_threshold': 0.75,  # Increased threshold for 4 models
            'min_models_required': 2,
            'model_specializations': self.get_model_specializations(),
            'model_weights': {k: v.confidence_weight for k, v in self.models.items()}
        }

    def store_learning_pattern(self, pattern_type: str, original_classification: str,
                             corrected_classification: str, reasoning: str, source_audit_id: str):
        """Store a new learning pattern in the database"""
        try:
            pattern = LearningPattern(
                id=f"pattern_{int(time.time())}_{hashlib.md5(f'{pattern_type}_{original_classification}'.encode()).hexdigest()[:8]}",
                pattern_type=pattern_type,
                contract_pattern="",  # Could be enhanced to store actual patterns
                vulnerability_type=original_classification,
                original_classification=original_classification,
                corrected_classification=corrected_classification,
                confidence_threshold=0.8,
                reasoning=reasoning,
                source_audit_id=source_audit_id,
                created_at=time.time(),
                usage_count=0,
                success_rate=1.0  # New pattern starts with perfect score
            )

            self.db_manager.store_learning_pattern(pattern)
            logger.info(f"Stored learning pattern: {pattern_type}")

        except Exception as e:
            logger.error(f"Failed to store learning pattern: {e}")


# Alias for backward compatibility
EnhancedAIEnsemble = AIEnsemble


