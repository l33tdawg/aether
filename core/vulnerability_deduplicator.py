"""
Vulnerability Deduplicator

This module deduplicates vulnerability findings that are reported multiple times
by different detectors or analyzers.

Key improvements from ZetaChain validation:
1. Merge duplicate findings on same line
2. Combine confidence scores intelligently
3. Preserve best description from multiple reports
4. Reduce noise in final reports
"""

import re
from typing import List, Dict, Any, Set, Tuple
from dataclasses import dataclass
from collections import defaultdict


@dataclass
class DuplicateGroup:
    """Represents a group of duplicate vulnerabilities"""
    vulnerabilities: List[Dict[str, Any]]
    merged_confidence: float
    best_description: str
    line_number: int
    vulnerability_type: str


class VulnerabilityDeduplicator:
    """Deduplicates vulnerability findings"""
    
    def __init__(self):
        self.similarity_threshold = 0.8
        
    def deduplicate(self, vulnerabilities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Deduplicate vulnerabilities based on multiple criteria
        
        Args:
            vulnerabilities: List of vulnerability dicts
            
        Returns:
            Deduplicated list with merged information
        """
        if not vulnerabilities:
            return []
        
        # Group by line number and vulnerability type
        groups = self._group_similar_vulnerabilities(vulnerabilities)
        
        # Merge each group into a single vulnerability
        deduplicated = []
        for group in groups:
            merged = self._merge_vulnerability_group(group)
            deduplicated.append(merged)
        
        return deduplicated
    
    def _group_similar_vulnerabilities(
        self,
        vulnerabilities: List[Dict[str, Any]]
    ) -> List[DuplicateGroup]:
        """Group similar vulnerabilities together"""
        groups: Dict[Tuple[int, str], List[Dict[str, Any]]] = defaultdict(list)
        
        for vuln in vulnerabilities:
            # Create key based on line number and vulnerability type
            line = vuln.get('line', vuln.get('line_number', 0))
            vuln_type = self._normalize_vulnerability_type(vuln.get('vulnerability_type', ''))
            
            key = (line, vuln_type)
            groups[key].append(vuln)
        
        # Create DuplicateGroup objects
        duplicate_groups = []
        for (line, vuln_type), vulns in groups.items():
            if len(vulns) > 1:
                # Multiple vulnerabilities at same location - likely duplicates
                group = DuplicateGroup(
                    vulnerabilities=vulns,
                    merged_confidence=self._calculate_merged_confidence(vulns),
                    best_description=self._select_best_description(vulns),
                    line_number=line,
                    vulnerability_type=vuln_type
                )
                duplicate_groups.append(group)
            else:
                # Single vulnerability - no merge needed
                group = DuplicateGroup(
                    vulnerabilities=vulns,
                    merged_confidence=vulns[0].get('confidence', 0.5),
                    best_description=vulns[0].get('description', ''),
                    line_number=line,
                    vulnerability_type=vuln_type
                )
                duplicate_groups.append(group)
        
        return duplicate_groups
    
    def _normalize_vulnerability_type(self, vuln_type: str) -> str:
        """Normalize vulnerability type for comparison"""
        # Map similar vulnerability types to canonical names
        type_map = {
            'missing_validation': 'input_validation',
            'missing_input_validation': 'input_validation',
            'input_validation': 'input_validation',
            'no_validation': 'input_validation',
            'send_without_validation': 'input_validation',
            'external_function_without_state_validation': 'input_validation',
            
            'missing_zero_check': 'zero_address',
            'zero_address': 'zero_address',
            'null_address': 'zero_address',
            'address_zero': 'zero_address',
            
            'access_control': 'access_control',
            'missing_access_control': 'access_control',
            'unauthorized_access': 'access_control',
        }
        
        normalized = vuln_type.lower().strip()
        return type_map.get(normalized, normalized)
    
    def _calculate_merged_confidence(self, vulnerabilities: List[Dict[str, Any]]) -> float:
        """
        Calculate merged confidence from multiple detections
        
        If multiple analyzers find the same issue, confidence increases.
        We use a formula that increases with detections but doesn't just average.
        """
        confidences = [v.get('confidence', 0.5) for v in vulnerabilities]
        
        if not confidences:
            return 0.5
        
        # If multiple detectors found it, boost confidence
        count = len(confidences)
        avg_confidence = sum(confidences) / count
        
        # Boost formula: More detectors = higher confidence, but with diminishing returns
        # Base: average confidence
        # Boost: +10% per additional detector (up to 3 detectors)
        boost = min((count - 1) * 0.1, 0.3)
        merged_confidence = min(avg_confidence + boost, 1.0)
        
        return round(merged_confidence, 2)
    
    def _select_best_description(self, vulnerabilities: List[Dict[str, Any]]) -> str:
        """Select the best description from multiple vulnerabilities"""
        descriptions = [v.get('description', '') for v in vulnerabilities]
        
        # Filter out vague descriptions
        vague_patterns = [
            r'^send without validation$',
            r'^external function without state validation$',
            r'^missing validation$',
            r'^no validation$'
        ]
        
        detailed_descriptions = []
        for desc in descriptions:
            is_vague = any(re.match(pattern, desc.lower().strip()) for pattern in vague_patterns)
            if not is_vague:
                detailed_descriptions.append(desc)
        
        # Use detailed description if available, otherwise use the longest one
        if detailed_descriptions:
            # Return the longest detailed description (likely has more context)
            return max(detailed_descriptions, key=len)
        else:
            # All descriptions are vague, return longest
            return max(descriptions, key=len) if descriptions else ''
    
    def _merge_vulnerability_group(self, group: DuplicateGroup) -> Dict[str, Any]:
        """Merge a group of similar vulnerabilities into one"""
        vulns = group.vulnerabilities
        
        # Start with the first vulnerability as base
        merged = vulns[0].copy()
        
        # Update with merged information
        merged['confidence'] = group.merged_confidence
        merged['description'] = group.best_description
        
        # Add note about multiple detections if applicable
        if len(vulns) > 1:
            merged['description'] += f" [Note: This vulnerability was detected by multiple analyzers, increasing confidence in the finding.]"
            merged['detection_count'] = len(vulns)
            merged['detected_by'] = [v.get('detector', 'unknown') for v in vulns]
        
        # Merge additional context
        contexts = []
        for v in vulns:
            if 'context' in v and v['context']:
                contexts.append(v['context'])
        if contexts:
            merged['context'] = contexts[0]  # Use first context
        
        # Use highest severity if there's disagreement
        severities = [v.get('severity', 'medium') for v in vulns]
        severity_order = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1, 'info': 0}
        max_severity = max(severities, key=lambda s: severity_order.get(s.lower(), 1))
        merged['severity'] = max_severity
        
        return merged
    
    def remove_subsumed_vulnerabilities(
        self,
        vulnerabilities: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Remove vulnerabilities that are subsumed by more specific ones
        
        Example: If we have both "missing validation" and "missing zero-address check"
        for the same function, keep only the more specific one.
        """
        # Group by function/line
        by_location = defaultdict(list)
        for vuln in vulnerabilities:
            line = vuln.get('line', vuln.get('line_number', 0))
            by_location[line].append(vuln)
        
        filtered = []
        for line, vulns in by_location.items():
            if len(vulns) == 1:
                filtered.append(vulns[0])
                continue
            
            # Check for subsumption
            # Keep more specific vulnerabilities, remove generic ones
            generic_types = {'missing_validation', 'input_validation', 'no_validation'}
            
            has_specific = any(
                self._normalize_vulnerability_type(v.get('vulnerability_type', '')) not in generic_types
                for v in vulns
            )
            
            if has_specific:
                # Remove generic vulnerabilities if we have specific ones
                for vuln in vulns:
                    vuln_type = self._normalize_vulnerability_type(vuln.get('vulnerability_type', ''))
                    if vuln_type not in generic_types:
                        filtered.append(vuln)
            else:
                # All are generic, keep all (will be deduplicated later)
                filtered.extend(vulns)
        
        return filtered

